{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, planes):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(planes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualNet(nn.Module):\n",
    "    def __init__(self, input_planes, height, width, number_of_blocks, classes):\n",
    "        super(ResidualNet, self).__init__()\n",
    "        \n",
    "        if number_of_blocks < 2:\n",
    "            raise ValueError(\"The residual net needs at least two blocks.\")\n",
    "        \n",
    "        self.conv1 = conv3x3(input_planes, 16)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.residual1 = ResidualBlock(16)\n",
    "        self.conv2 = conv3x3(16, 32, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.residual2 = ResidualBlock(32)\n",
    "        self.conv3 = conv3x3(32, 64, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.laterResidualBlocks = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(number_of_blocks-2):\n",
    "            self.laterResidualBlocks.append(ResidualBlock(64))\n",
    "        \n",
    "        self.dense_input_dim = height * width * 4\n",
    "        self.dense = nn.Linear(self.dense_input_dim, classes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.residual1(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.residual2(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        for block in self.laterResidualBlocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        return self.dense(x.view(-1, self.dense_input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "class AdvancedImageFolder(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None,\n",
    "                 loader=default_loader, filter_fn=None, shuffle=False):\n",
    "        super(AdvancedImageFolder, self).__init__(root, transform, target_transform, loader)\n",
    "        \n",
    "        if filter_fn is not None:\n",
    "            self.imgs = list(filter(filter_fn, self.imgs))\n",
    "        \n",
    "        if shuffle:\n",
    "            random.shuffle(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tightest_image_crop(img, preserve_aspect_ratio=False):\n",
    "    image_indices = F.threshold(Variable(img[0]), 0.0000001, 0).data.nonzero()\n",
    "    top_i = image_indices[0,0]\n",
    "    bottom_i = image_indices[-1,0]\n",
    "    \n",
    "    mins, _ = image_indices.min(dim=0)\n",
    "    left_i = mins[1]\n",
    "    \n",
    "    maxs, _ = image_indices.max(dim=0)\n",
    "    right_i = maxs[1]\n",
    "    \n",
    "    new_width = right_i-left_i+1\n",
    "    new_height = top_i-bottom_i+1\n",
    "        \n",
    "    if preserve_aspect_ratio:\n",
    "        if new_width > new_height:\n",
    "            result = img[:, top_i:top_i+new_width, left_i:right_i+1]\n",
    "            show(result)\n",
    "            plt.show()\n",
    "            return img[:, top_i:top_i+new_width, left_i:right_i+1]\n",
    "        else:\n",
    "            result = img[:, top_i:bottom_i+1, left_i:left_i+new_height]\n",
    "            show(result)\n",
    "            plt.show()\n",
    "            return img[:, top_i:bottom_i+1, left_i:left_i+new_height]\n",
    "        \n",
    "    return img[:, top_i:bottom_i+1, left_i:right_i+1]\n",
    "\n",
    "def square_padding(img):\n",
    "    _, height, width = img.size()\n",
    "    \n",
    "    if height > width:\n",
    "        padding = torch.zeros(1, height, height-width)\n",
    "        return torch.cat((padding, img), 2)\n",
    "    elif width > height:\n",
    "        padding = torch.zeros(1, width-height, width)\n",
    "        return torch.cat((padding, img), 1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def image_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-06d573b3e49f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transform = transforms.Compose([\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtightest_image_crop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare_padding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda img: 1 - img),\n",
    "        transforms.Lambda(tightest_image_crop),\n",
    "        transforms.Lambda(square_padding),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dset, batch_size=128, thread_count=4):\n",
    "    sampler_dset_train = data.sampler.SubsetRandomSampler(list(range(int(0.7*len(dset)))))\n",
    "    sampler_dset_test = data.sampler.SubsetRandomSampler(list(range(int(0.7*len(dset)), \n",
    "                                                                    int(0.85*len(dset)))))\n",
    "    sampler_dset_validation = data.sampler.SubsetRandomSampler(list(range(int(0.85*len(dset)), \n",
    "                                                                          len(dset))))\n",
    "\n",
    "    loader_dset_train = torch.utils.data.DataLoader(\n",
    "        dset, batch_size=batch_size, num_workers=thread_count,\n",
    "        pin_memory=True, sampler = sampler_dset_train)\n",
    "    loader_dset_test = torch.utils.data.DataLoader(\n",
    "        dset, batch_size=batch_size, num_workers=thread_count,\n",
    "        pin_memory=True, sampler = sampler_dset_test)\n",
    "    loader_dset_validation = torch.utils.data.DataLoader(\n",
    "        dset, batch_size=batch_size, num_workers=thread_count,\n",
    "        pin_memory=True, sampler = sampler_dset_validation)\n",
    "\n",
    "    return loader_dset_train, loader_dset_test, loader_dset_validation\n",
    "\n",
    "dset_type = AdvancedImageFolder('by_class', transform, \n",
    "                                target_transform = lambda n: 0 if n < 10 else 1, \n",
    "                                loader = image_loader,\n",
    "                                shuffle = True)\n",
    "dset_digit = AdvancedImageFolder('by_class', transform, loader = image_loader,\n",
    "                                 filter_fn = lambda p: p[1] < 10,\n",
    "                                 shuffle = True)\n",
    "dset_char = AdvancedImageFolder('by_class', transform, target_transform = lambda n: n - 10, \n",
    "                                loader = image_loader, filter_fn = lambda p: p[1] >= 10,\n",
    "                                shuffle = True)\n",
    "\n",
    "dset_uppercase_char = AdvancedImageFolder('by_class', transform, target_transform = lambda n: n - 10, \n",
    "                                loader = image_loader, filter_fn = lambda p: p[1] >= 10 and p[1] <= 35,\n",
    "                                shuffle = True)\n",
    "\n",
    "loader_type_train, loader_type_test, _ = split_dataset(dset_type)\n",
    "loader_digit_train, loader_digit_test, _ = split_dataset(dset_digit)\n",
    "loader_char_train, loader_char_test, _ = split_dataset(dset_char)\n",
    "loader_uppercase_char_train, loader_uppercase_char_test, _ = split_dataset(dset_uppercase_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_type = ResidualNet(1, 32, 32, 4, 2)\n",
    "resnet_digit = ResidualNet(1, 32, 32, 6, 10)\n",
    "resnet_char = ResidualNet(1, 32, 32, 2, 52)\n",
    "resnet_uppercase_char = ResidualNet(1, 32, 32, 3, 26)\n",
    "\n",
    "resnet_type = nn.DataParallel(resnet_type.cuda())\n",
    "resnet_digit = nn.DataParallel(resnet_digit.cuda())\n",
    "resnet_char = nn.DataParallel(resnet_char.cuda())\n",
    "resnet_uppercase_char = nn.DataParallel(resnet_uppercase_char.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dset_loader, criterion, optimizer, lr_scheduler, num_epochs=5):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "    model.train(True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        optimizer = lr_scheduler(optimizer, epoch)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        current_batch = 0\n",
    "        # Iterate over data.\n",
    "        for data in dset_loader:\n",
    "            current_batch += 1\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs.cuda()), \\\n",
    "                             Variable(labels.cuda())\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.data[0]\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if current_batch % 20 == 0:\n",
    "                curr_acc = running_corrects / (current_batch * dset_loader.batch_size)\n",
    "                curr_loss = running_loss / (current_batch * dset_loader.batch_size)\n",
    "                time_elapsed = time.time() - since\n",
    "\n",
    "                print('Epoch Number: {}, Batch Number: {}, Loss: {:.4f}, Acc: {:.4f}'.format(\n",
    "                        epoch, current_batch, curr_loss, curr_acc))\n",
    "                print('Time so far is {:.0f}m {:.0f}s'.format(\n",
    "                      time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "        epoch_loss = running_loss / (len(dset_loader) * dset_loader.batch_size) \n",
    "        epoch_acc = running_corrects / (len(dset_loader) * dset_loader.batch_size) \n",
    "\n",
    "        # deep copy the model\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.train(False)\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_lr_scheduler(optimizer, epoch, init_lr=0.001, lr_decay_epoch=7):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (0.1**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_type = optim.SGD(resnet_type.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_digit = optim.SGD(resnet_digit.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_char = optim.SGD(resnet_char.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_uppercase_char = optim.SGD(resnet_uppercase_char.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dset_loader):\n",
    "    model.train(False)\n",
    "    \n",
    "    running_corrects = 0\n",
    "    \n",
    "    for data in dset_loader:\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs.cuda()), \\\n",
    "                         Variable(labels.cuda())\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    return running_corrects/(len(dset_loader) * dset_loader.batch_size) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
