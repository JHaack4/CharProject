{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'Text Recognition 1.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "import seaborn\n",
    "import string\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_directory = os.path.expanduser('~')\n",
    "nn_library_path = home_directory + '/Documents/HarveyMuddWork/Neural_Nets_Research/neural_nets_research/Neural Nets Library'\n",
    "sys.path.append(nn_library_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_type = ResidualNet(1, 32, 32, 3, 2)\n",
    "resnet_digit = ResidualNet(1, 32, 32, 3, 10)\n",
    "resnet_uppercase_char = ResidualNet(1, 32, 32, 3, 26)\n",
    "\n",
    "resnet_type = nn.DataParallel(resnet_type.cuda())\n",
    "resnet_digit = nn.DataParallel(resnet_digit.cuda())\n",
    "resnet_uppercase_char = nn.DataParallel(resnet_uppercase_char.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_validation(model, train_loader, validation_loader, criterion, \n",
    "                                optimizer, lr_scheduler, num_epochs=20):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train(True)\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        optimizer = lr_scheduler(optimizer, epoch)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        current_batch = 0\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in train_loader:\n",
    "            current_batch += 1\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs.cuda()), \\\n",
    "                             Variable(labels.cuda())\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.data[0]\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if current_batch % 250 == 0:\n",
    "                curr_acc = running_corrects / (current_batch * train_loader.batch_size)\n",
    "                curr_loss = running_loss / (current_batch * train_loader.batch_size)\n",
    "                time_elapsed = time.time() - since\n",
    "\n",
    "                print('Epoch Number: {}, Batch Number: {}, Loss: {:.4f}, Acc: {:.4f}'.format(\n",
    "                        epoch, current_batch, curr_loss, curr_acc))\n",
    "                print('Time so far is {:.0f}m {:.0f}s'.format(\n",
    "                      time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "                \n",
    "        \n",
    "        validation_acc = test_model(model, validation_loader)\n",
    "        print('Epoch Number: {}, Validation Accuracy: {:.4f}'.format(epoch, validation_acc))\n",
    "\n",
    "        # deep copy the model\n",
    "        if validation_acc > best_acc:\n",
    "            best_acc = validation_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.train(False)\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dset, batch_size=128, thread_count=4):\n",
    "    sampler_dset_train = data.sampler.SubsetRandomSampler(list(range(int(0.7*len(dset)))))\n",
    "    sampler_dset_test = data.sampler.SubsetRandomSampler(list(range(int(0.7*len(dset)), \n",
    "                                                                    int(0.85*len(dset)))))\n",
    "    sampler_dset_validation = data.sampler.SubsetRandomSampler(list(range(int(0.85*len(dset)), \n",
    "                                                                          len(dset))))\n",
    "\n",
    "    loader_dset_train = torch.utils.data.DataLoader(\n",
    "        dset, batch_size=batch_size, num_workers=thread_count,\n",
    "        pin_memory=True, sampler = sampler_dset_train)\n",
    "    loader_dset_test = torch.utils.data.DataLoader(\n",
    "        dset, batch_size=batch_size, num_workers=thread_count,\n",
    "        pin_memory=True, sampler = sampler_dset_test)\n",
    "    loader_dset_validation = torch.utils.data.DataLoader(\n",
    "        dset, batch_size=batch_size, num_workers=thread_count,\n",
    "        pin_memory=True, sampler = sampler_dset_validation)\n",
    "\n",
    "    return loader_dset_train, loader_dset_test, loader_dset_validation\n",
    "\n",
    "dset_type = AdvancedImageFolder('by_class', transform, \n",
    "                                target_transform = lambda n: 0 if n < 10 else 1, \n",
    "                                loader = image_loader,\n",
    "                                filter_fn = lambda p: p[1] <= 35,\n",
    "                                shuffle = True)\n",
    "dset_digit = AdvancedImageFolder('by_class', transform, loader = image_loader,\n",
    "                                 filter_fn = lambda p: p[1] < 10,\n",
    "                                 shuffle = True)\n",
    "dset_uppercase_char = AdvancedImageFolder('by_class', transform, target_transform = lambda n: n - 10, \n",
    "                                loader = image_loader, filter_fn = lambda p: p[1] >= 10 and p[1] <= 35,\n",
    "                                shuffle = True)\n",
    "\n",
    "loader_type_train, loader_type_test, loader_type_validation = split_dataset(dset_type, thread_count=8, batch_size=128)\n",
    "loader_digit_train, loader_digit_test, loader_digit_validation = split_dataset(dset_digit, thread_count=8, batch_size=128)\n",
    "loader_uppercase_char_train, loader_uppercase_char_test, loader_uppercase_char_validation = split_dataset(dset_uppercase_char, thread_count=8, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_type = optim.SGD(resnet_type.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_digit = optim.SGD(resnet_digit.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_uppercase_char = optim.SGD(resnet_uppercase_char.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_type = train_model_with_validation(resnet_type, loader_type_train, loader_type_validation, criterion, \n",
    "                                          optimizer_type, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_model(resnet_type, loader_type_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_uppercase_char = train_model_with_validation(resnet_uppercase_char, loader_uppercase_char_train, \n",
    "                                                    loader_uppercase_char_validation, criterion, \n",
    "                                                    optimizer_uppercase_char, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_model(resnet_uppercase_char, loader_uppercase_char_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_digit = train_model_with_validation(resnet_digit, loader_digit_train, loader_digit_validation, criterion, \n",
    "                                           optimizer_digit, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_model(resnet_digit, loader_digit_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    _, category_i = output.data.max(1) # Tensor out of Variable with .data\n",
    "    return category_i\n",
    "\n",
    "def categoryAndProb(output):\n",
    "    top_value, category_i = output.data.max(1) # Tensor out of Variable with .data\n",
    "    return top_value, category_i\n",
    "\n",
    "def confusion_matrix(model, data_loader, all_categories):\n",
    "    n_categories = len(all_categories)\n",
    "    # Keep track of correct guesses in a confusion matrix\n",
    "    confusion = torch.zeros(n_categories, n_categories)\n",
    "   \n",
    "    for inputs, labels in data_loader:\n",
    "        inputs = Variable(inputs.cuda())\n",
    "        output = model(inputs)\n",
    "        \n",
    "        guesses = categoryFromOutput(output)\n",
    "        \n",
    "        for category_i, guess_i in zip(labels, guesses):\n",
    "            confusion[category_i][guess_i] += 1\n",
    "\n",
    "    # Normalize by dividing every row by its sum\n",
    "    for i in range(n_categories):\n",
    "        confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "    # Set up plot\n",
    "    fig = plt.figure(figsize = (16,16), dpi = 160)\n",
    "    ax = fig.add_subplot(111)\n",
    "    print(confusion)\n",
    "    cax = ax.matshow(confusion.numpy())\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "    ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "    # Force label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    # sphinx_gallery_thumbnail_number = 2\n",
    "    plt.show()\n",
    "    \n",
    "def accuracy_for_each_category(model, data_loader, all_categories):\n",
    "    n_categories = len(all_categories)\n",
    "    \n",
    "    # Keep track of guesses\n",
    "    number_of_corrects = torch.zeros(n_categories)\n",
    "    number_of_appearances = torch.zeros(n_categories)\n",
    "    \n",
    "    for inputs, labels in data_loader:\n",
    "        inputs = Variable(inputs.cuda())\n",
    "        output = model(inputs)\n",
    "        \n",
    "        guesses = categoryFromOutput(output)\n",
    "        \n",
    "        for category_i, guess_i in zip(labels, guesses):\n",
    "            if category_i == guess_i:\n",
    "                number_of_corrects[category_i] += 1\n",
    "            \n",
    "            number_of_appearances[category_i] += 1\n",
    "        \n",
    "    accuracies = number_of_corrects / number_of_appearances\n",
    "    \n",
    "    return dict(zip(all_categories, accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = list(string.ascii_uppercase)\n",
    "digits = list(range(10))\n",
    "all_symbols = digits + characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(resnet_type, loader_type_test, ['digit', 'character'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(resnet_uppercase_char, loader_uppercase_char_test,\n",
    "                 characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(resnet_digit, loader_digit_test, digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_all = AdvancedImageFolder('by_class', transform, \n",
    "                                loader = image_loader,\n",
    "                                filter_fn = lambda p: p[1] <= 35,\n",
    "                                shuffle = True)\n",
    "\n",
    "loader_all_train, loader_all_test, loader_all_validation = split_dataset(dset_all, thread_count=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_all = ResidualNet(1, 32, 32, 5, 36)\n",
    "resnet_all = nn.DataParallel(resnet_all.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_all = optim.SGD(resnet_all.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_all = train_model_with_validation(resnet_all, loader_all_train, loader_all_validation, criterion, \n",
    "                                         optimizer_all, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_model(resnet_all, loader_all_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(resnet_all, loader_all_test, all_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwritingClassifier(nn.Module):\n",
    "    def __init__(self, char_model, digit_model, type_model, with_joint = False):\n",
    "        super(HandwritingClassifier, self).__init__()\n",
    "        self.char_model = char_model\n",
    "        self.digit_model = digit_model\n",
    "        self.type_model = type_model\n",
    "        self.with_joint = with_joint\n",
    "    \n",
    "    def forward(self, x):\n",
    "        type_values = F.log_softmax(self.type_model(x))\n",
    "        char_values = F.log_softmax(self.char_model(x)) \n",
    "        digit_values = F.log_softmax(self.digit_model(x))\n",
    "        \n",
    "        if self.with_joint:\n",
    "            log_prob_digits, log_prob_chars = type_values[:, 0].unsqueeze(1), type_values[:, 1].unsqueeze(1)\n",
    "            \n",
    "            digit_log_probs = digit_values + log_prob_digits\n",
    "            char_log_probs = char_values + log_prob_chars\n",
    "        else:\n",
    "            _, top_types = type_values.max(1)\n",
    "            top_types = top_types.unsqueeze(1).float()\n",
    "                        \n",
    "            digit_log_probs = digit_values * (top_types - 1)\n",
    "            char_log_probs = char_values * top_types\n",
    "\n",
    "        return torch.cat((digit_log_probs, char_log_probs), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_handwriting_classifiers(char_model, digit_model, type_model, dset_loader, with_joint = False):\n",
    "    char_model.train(False)\n",
    "    digit_model.train(False)\n",
    "    type_model.train(False)\n",
    "    \n",
    "    running_corrects = 0\n",
    "    \n",
    "    for inputs, labels in dset_loader:\n",
    "        inputs, labels = Variable(inputs.cuda()), \\\n",
    "                         Variable(labels.cuda())\n",
    "        types = type_model(inputs)\n",
    "            \n",
    "        if with_joint:\n",
    "            char_values = F.softmax(char_model(inputs)) \n",
    "            digit_values = F.softmax(digit_model(inputs))\n",
    "            type_values = F.softmax(types)\n",
    "            \n",
    "            prob_digits, prob_chars = type_values[:, 0].unsqueeze(1), type_values[:, 1].unsqueeze(1)\n",
    "            \n",
    "            char_probs = char_values * prob_chars\n",
    "            digit_probs = digit_values * prob_digits\n",
    "\n",
    "            top_char_prob, top_char_index = categoryAndProb(char_probs)\n",
    "            top_digit_prob, top_digit_index = categoryAndProb(digit_probs)\n",
    "        \n",
    "            possible_chars = top_char_prob >= top_digit_prob\n",
    "            possible_digits = top_digit_prob > top_char_prob\n",
    "            \n",
    "            char_guesses, label_chars = top_char_index[possible_chars], labels[possible_chars] - 10\n",
    "            digit_guesses, label_digits = top_digit_index[possible_digits], labels[possible_digits]\n",
    "            \n",
    "            running_corrects += torch.sum(char_guesses == label_chars.data)\n",
    "            running_corrects += torch.sum(digit_guesses == label_digits.data)\n",
    "        else:\n",
    "            top_types = categoryFromOutput(types)\n",
    "            \n",
    "            possible_chars = top_types.nonzero().squeeze()\n",
    "            possible_digits = (top_types - 1).nonzero().squeeze()\n",
    "            \n",
    "            input_chars, label_chars = inputs[possible_chars], labels[possible_chars] - 10\n",
    "            input_digits, label_digits = inputs[possible_digits], labels[possible_digits]\n",
    "\n",
    "            char_guesses = categoryFromOutput(char_model(input_chars))\n",
    "            digit_guesses = categoryFromOutput(digit_model(input_digits))\n",
    "            \n",
    "            running_corrects += torch.sum(char_guesses == label_chars.data)\n",
    "            running_corrects += torch.sum(digit_guesses == label_digits.data)\n",
    "    \n",
    "    return running_corrects/(len(dset_loader) * dset_loader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model1, model2, dset_loader):\n",
    "    running_matches = 0\n",
    "    \n",
    "    for inputs, labels in dset_loader:\n",
    "        # wrap them in Variable\n",
    "        inputs = Variable(inputs.cuda())\n",
    "\n",
    "        # forward\n",
    "        outputs1 = model1(inputs)\n",
    "        _, preds1 = outputs1.data.max(1)\n",
    "        \n",
    "        outputs2 = model2(inputs)\n",
    "        _, preds2 = outputs2.data.max(1)\n",
    "        \n",
    "        print(preds1)\n",
    "        print(preds2)\n",
    "        print(labels)\n",
    "        \n",
    "        running_matches += torch.sum(preds1 == preds2)\n",
    "        print(running_matches)\n",
    "        raise ValueError('Hi')\n",
    "    \n",
    "    return running_matches/(len(dset_loader) * dset_loader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_point(model, datapoint, class_names=None):\n",
    "    inputs = Variable(datapoint.cuda()).unsqueeze(0)    \n",
    "    output = model(inputs)\n",
    "        \n",
    "    _, preds = output.data.max(1)\n",
    "    preds = preds[0]\n",
    "    \n",
    "    if class_names is None:\n",
    "        return preds\n",
    "    else:\n",
    "        return class_names[preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_model_acc_no_joint = test_handwriting_classifiers(resnet_uppercase_char, resnet_digit, resnet_type, loader_all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_model_acc_no_joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_model_acc_with_joint = test_handwriting_classifiers(resnet_uppercase_char, resnet_digit, resnet_type, loader_all_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_model_acc_with_joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model = HandwritingClassifier(resnet_uppercase_char, resnet_digit, resnet_type, with_joint=True)\n",
    "nonjoint_model = HandwritingClassifier(resnet_uppercase_char, resnet_digit, resnet_type, with_joint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_model(nonjoint_model, loader_all_test))\n",
    "print(test_model(joint_model, loader_all_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(joint_model, loader_all_test, all_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict = accuracy_for_each_category(joint_model, loader_all_test, all_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol, accuracy in sorted(accuracy_dict.items(), key = lambda p: p[1]):\n",
    "    print('Symbol: {}, Acc: {:.4f}'.format(symbol, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict_chars = accuracy_for_each_category(resnet_uppercase_char, loader_uppercase_char_test, characters)\n",
    "accuracy_dict_digits = accuracy_for_each_category(resnet_digit, loader_digit_test, digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for char, accuracy in sorted(accuracy_dict_chars.items(), key = lambda p: p[1]):\n",
    "    print('Letter: {}, Acc: {:.4f}'.format(char, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for digit, accuracy in sorted(accuracy_dict_digits.items(), key = lambda p: p[1]):\n",
    "    print('Letter: {}, Acc: {:.4f}'.format(digit, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_models(joint_model, nonjoint_model, loader_all_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform2 = transforms.Compose((transforms.ToTensor(),\n",
    "                                 transforms.Lambda(tightest_image_crop),\n",
    "                                 transforms.Lambda(square_padding),\n",
    "                                 transforms.ToPILImage(),\n",
    "                                 transforms.Scale(32),\n",
    "                                 transforms.ToTensor()\n",
    "                                 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_cropped_images = AdvancedImageFolder('Cropped Characters', transform2, \n",
    "                                          loader = image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dset_cropped_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img):\n",
    "    npimg = img.numpy()[0]\n",
    "    plt.figure()\n",
    "    plt.imshow(npimg, interpolation='nearest', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Also fix non joint model.\n",
    "\n",
    "# 2. One thing to see if it matters is making it so that the non joint model only uses the appropriate\n",
    "# second model. Not sure if it'll have a noticeable impact on efficiency.\n",
    "\n",
    "# 3. Also should try training these models end to end.\n",
    "\n",
    "# 4. Also should make function that prints out the rank of the correct answer when wrong.\n",
    "\n",
    "# 5. Pad on both sides instead of just padding to left/top.\n",
    "\n",
    "# 6. Cut out characters from street name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = joint_model(Variable(dset_cropped_images[0][0].unsqueeze(0)).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_visual = make_dot(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(graph_visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_visual.render('joint_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
